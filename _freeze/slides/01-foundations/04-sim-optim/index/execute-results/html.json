{
  "hash": "2266e6375ce4652e273daa27cd4b74f9",
  "result": {
    "markdown": "---\ntitle: Simulation and Optimisation\nsubtitle: R Foundations course\nauthor: Ella Kaye | Department of Statistics | University of Warwick\ntitle-slide-attributes:\n  data-background-color: \"#552D62\"\ndate: 2022-11-07\ndate-format: long\nformat: \n  revealjs:\n    theme: [default, ../../slides.scss]\n    slide-number: true\n    execute:\n      echo: true\n      code-overflow: wrap\n---\n\n\n## Overview\n\n- Numerical precision\n\n- Simulation\n\n- Optimisation\n\n# Numerical precision {background-color=\"#552D62\"}\n\n# Simulation {background-color=\"#552D62\"}\n\n## Random numbers\n\n## `set.seed()`\n\n## `sample()`\n\n## Distributions in R\n\n## Random samples from distributions\n\n# Optimisation {background-color=\"#552D62\"}\n\nSlides below from [Ruth Ripley's APTS course](https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf)\n\n## The optimisation problem\n\n- Given a function $f(x)$, what value of $x$ makes $f(x)$ as small or large as possible?\n- In a statistical context, $x$ will usually be the parameters of a model, and $f(x)$ either the model likelihood to be maximised or some measure of discrepancy between data and predictions to be minimised.\n- The optimal set of parameters will give the *best fit*\n- Only need to consider *small* as $-f(x)$ is *large* when $f(x)$ is *small*.\n- We consider here *general-purpose optimisers*\n\n## Local and Global Minima\n\n- The (negative of the) likelihood for the General Linear Model (and that for many other linear models) is well-behaved: it has a single, global minimum.\n- For more complicated models there may be many local minima.\n- Finding a global minimum is difficult, and not always important. Only if local minima are widely separated in parameter space are they likely to invalidate our conclusions.\n- We will concentrate on methods of finding local minima.\nCheck for different local minima by altering the initial values, algorithm used, or other parameters of the fitting process.\n\n## Univariate Optimisation\n\n`optimize` (or `optimise`) finds a (possibly local) mimimum of a function in a specified interval with respect to its first argument.\n\n- Function to be minimised is the first argument to `optimize`\n- Can pre-specify the function or include it in the command.\n\n## Univariate Optimisation: example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x, a) {\n  (x - a)^2\n}\n\nxmin <- optimize(f, interval = c(0, 1), a = 1/3)\n\n# or\n\nxmin <- optimize(function(x, a) {(x - a)^2}, \n                 interval = c(0, 1), a = 1/3)\n\nxmin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$minimum\n[1] 0.3333333\n\n$objective\n[1] 0\n```\n:::\n:::\n\nNote how the (fixed) parameter `a` is passed into `f`.\n\n## Other `optimize()` arguments\n\n- An interval within which to search must be specified, either by `interval` or by `upper` and `lower`\n\n- To *maximise* set `maximum = TRUE`\n\n- Accuracy can be set using the `tol` argument\n\n- Note the order of arguments: `optimize(f, interval, ..., lower, upper, maximum, tol)\n\n- The `...` can be named or unnamed and will be passed to `f`\n\n- Arguments after the `...` must be specified by names.\n\n- `optimize` returns a list with two items:\n\n  - `minimum`: the value of `x` at which `f(x)` is minimised\n  - `objective`: the value of `f(x)` at `x = minimum` \n  \n## General Optimisation\n\n- In more than one dimension the problem is harder.\n- R has several different functions: most flexible is `optim()` which includes several different algorithms.\n- Algorithm of choice depends on how easy it is to calculate derivatives for the function. Usually better to supply a function to calculate derivatives, but may be unnecessary extra work.\n- Ensure the problem is scaled so that unit change in any parameter gives approximately unit change in objective.\n\n## `optim()` methods\n\n::: {.panel-tabset}\n## Nelder-Mead\n- The default method\n- Basic idea: for a function with `n` parameters, choose a polygon with `n+1` vertices. At each step, alter vertex with minimum `f(x)` to improve the objective function, by *reflection*, *expansion* or *contraction*\n- Does not use derivative information \n- Useful for non-differentiable functions \n- May be rather slow\n\n## BFGS\n- A quasi-Newton method: builds up approximation to Hessian matrix from gradients at start and finish of steps\n- Uses the approximation to choose new search direction \n- Performs line search in this direction\n- Update term for the Hessian approximation is due to Broyden, Fletcher, Goldfarb and Shanno (proposed separately by all four in 1970)\n- Uses derivative information, calculated either from a user-supplied function or by finite differences\n- If dimension is large, the matrix stored may be very large\n\n## CG method\n- A conjugate gradient method: chooses successive search directions that are analogous to axes of an ellipse\n- Does not store a Hessian matrix\n- Three different formulae for the search directions are implemented: Fletcher-Reeves, Polak-Ribiere or Beale-Sorenson\n- Less robust than BFGS method\n- Uses derivative information, calculated either from a user-supplied function or by finite differences\n\n## L-BFGS-B\n- A limited memory version of BFGS\n- Does not store a Hessian matrix, only a limited number of update steps for it\n- Uses derivative information, calculated either from a user-supplied function or by finite differences\n- Can restrict the solution to lie within a \"box\", the only method of `optim()` that can do this\n\n## SANN\n- A variant of simulated annealing A stochastic algorithm\n- Accepts changes which increase the objective with positive probability (when minimising!)\n- Does not use derivative information\n- Can be very slow to converge but may find a ‘good’ solution quickly\n\n## Brent\n- An interface to `optimize()`\n- Only for use with one dimensional problems\n- Included for use inside other functions where only method can be specified, not the function to be used.\n:::\n\n## How to use `optim()`\n\n`optim(par, fn, gr=NULL, ..., method=c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\", \"Brent\"), lower=-Inf, upper=Inf, control=list(), hessian=FALSE)`\n\n. . . \n\n`par`: starting values of the parameters\n`fn`: function to be optimised (supply as for `optimize`)\n`gr`: function to calculate the \n\n\n\n# End matter {background-color=\"#552D62\"}\n\n## Resources\n\nThis material is reproduced in large part from the APTS 2013/14 resources by Ruth Ripley:\n\n- https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf\n\n## License\n\nLicensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License ([CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/){target=\"_blank\"}).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}