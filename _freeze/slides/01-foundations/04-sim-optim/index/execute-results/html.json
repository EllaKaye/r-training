{
  "hash": "58f14e3db1d05ac22ee4ea6dee5b30f2",
  "result": {
    "markdown": "---\ntitle: Simulation and Optimisation\nsubtitle: R Foundations course\nauthor: Ella Kaye | Department of Statistics | University of Warwick\ntitle-slide-attributes:\n  data-background-color: \"#552D62\"\ndate: 2022-11-07\ndate-format: long\nformat: \n  revealjs:\n    theme: [default, ../../slides.scss]\n    slide-number: true\n    execute:\n      echo: true\n      code-overflow: wrap\n---\n\n\n## Overview\n\n-   Numerical precision\n\n-   Random numbers\n\n-   Simulation\n\n-   Optimisation\n\n# Numerical precision {background-color=\"#552D62\"}\n\n# Random numbers {background-color=\"#552D62\"}\n\n## Random number generation\n\n- Random numbers caluclated on a computer are not random.\n\n- They are *pseudo-random*, following a predicted sequence, but in the short-term (i.e. anything but the *very* long-term) will appear to be random.\n\n- This is useful, as two sets of random numbers of the same size from the same generator using the same initial value will be exactly the same. **This is crucial for reproducibility.**\n\n- R provides functions for generating random samples from standard distibutions.\n\n## Using Random Number Generators in R\n\n- Each time a random number is required, R will use and update a variable called `.Random.seed` which is in your workspace.\n- At the first use, if `.Random.seed` does not exist, one will be created, with a value generated from the time.\n- The function `set.seed(n)` will set `.Random.seed` to a value derived from the argument `n`. \n- Use `set.seed()` to repeat the same sequence of random numbers.\n- There are various different generators available. R will use the one in use at the start of your session unless you alter it, even if you delete `.Random.seed`. For safety, you can specify the kind on calls to set.seed. Use `set.seed(n, kind=\"default\")` to ensure you are using Râ€™s default.\n- It is possible to save and restore `.Random.seed` within your functions, but take care with scope (see next term!).\n\n## `set.seed()` example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  5  1  9  3  7  6  2  4  8 10\n```\n:::\n\n```{.r .cell-code}\nset.seed(1)\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  9  4  7  1  2  5  3 10  6  8\n```\n:::\n\n```{.r .cell-code}\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  3  1  5  8  2  6 10  9  4  7\n```\n:::\n\n```{.r .cell-code}\nset.seed(1)\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  9  4  7  1  2  5  3 10  6  8\n```\n:::\n:::\n\n\n\n## `sample()`\n\nThe `sample()` function re-samples from a data vector, with or without replacement.\n\n::: {.panel-tabset}\n## `sample(n)`\n\nA random permutation of 1, ..., n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nsample(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  3 10  2  8  6  9  1  7  5  4\n```\n:::\n:::\n\n\n\n## `sample(x)`\n\nA random permutation of `x` for `length(x) > 1`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\nalph10 <- LETTERS[1:10]\nsample(alph10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"I\" \"G\" \"H\" \"F\" \"C\" \"B\" \"J\" \"E\" \"D\" \"A\"\n```\n:::\n:::\n\n\n## sample(x, replace = TRUE)\n\nA bootstrap sample\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\nsample(alph10, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"I\" \"J\" \"G\" \"H\" \"F\" \"G\" \"C\" \"H\" \"J\" \"G\"\n```\n:::\n:::\n\n\n## sample(x, n)\n\nSample `n` items from `x` without replacement\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10)\nsample(alph10, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"I\" \"G\" \"H\" \"F\" \"C\" \"B\"\n```\n:::\n:::\n\n\n## sample(x, n, replace = TRUE)\n\nSample `n` items from `x` with replacement\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nsample(alph10, 6, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"I\" \"D\" \"G\" \"A\" \"B\" \"G\"\n```\n:::\n:::\n\n\n## sample(x, n, replace = TRUE, prob = probs)\n\nProbability sample of `n` items from `x`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 1:4\nprobs <- c(1/2, 1/3, 1/12, 1/12)\n\nset.seed(1)\nsamp <- sample(x, 100, replace = TRUE, prob = probs)\nsamp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 1 1 2 3 1 3 4 2 2 1 1 1 2 1 2 1 2 4 1 2 4 1 2 1 1 1 1 1 3 1 1 2 1 1 2 2 2\n [38] 1 2 1 2 2 2 2 2 2 1 1 2 2 1 3 1 1 1 1 1 2 2 1 3 1 1 1 2 1 1 2 1 3 1 3 1 1\n [75] 1 3 3 1 2 4 1 2 1 1 2 1 2 1 1 1 1 1 2 3 2 2 1 1 2 2\n```\n:::\n\n```{.r .cell-code}\ntable(samp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsamp\n 1  2  3  4 \n52 34 10  4 \n```\n:::\n:::\n\n\n\n:::\n\n\n\n# Simulation {background-color=\"#552D62\"}\n\n## Uses of Simulation\n\n## Distributions in R\n\n## Random samples from distributions\n\n# Optimisation {background-color=\"#552D62\"}\n\nSlides below from [Ruth Ripley's APTS course](https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf)\n\n## The optimisation problem\n\n-   Given a function $f(x)$, what value of $x$ makes $f(x)$ as small or large as possible?\n-   In a statistical context, $x$ will usually be the parameters of a model, and $f(x)$ either the model likelihood to be maximised or some measure of discrepancy between data and predictions to be minimised.\n-   The optimal set of parameters will give the *best fit*\n-   Only need to consider *small* as $-f(x)$ is *large* when $f(x)$ is *small*.\n-   We consider here *general-purpose optimisers*\n\n## Local and Global Minima\n\n-   The (negative of the) likelihood for the General Linear Model (and that for many other linear models) is well-behaved: it has a single, global minimum.\n-   For more complicated models there may be many local minima.\n-   Finding a global minimum is difficult, and not always important. Only if local minima are widely separated in parameter space are they likely to invalidate our conclusions.\n-   We will concentrate on methods of finding local minima. Check for different local minima by altering the initial values, algorithm used, or other parameters of the fitting process.\n\n## Univariate Optimisation\n\n`optimize` (or `optimise`) finds a (possibly local) mimimum of a function in a specified interval with respect to its first argument.\n\n-   Function to be minimised is the first argument to `optimize`\n-   Can pre-specify the function or include it in the command.\n\n## Univariate Optimisation: example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x, a) {\n  (x - a)^2\n}\n\nxmin <- optimize(f, interval = c(0, 1), a = 1/3)\n\n# or\n\nxmin <- optimize(function(x, a) {(x - a)^2}, \n                 interval = c(0, 1), a = 1/3)\n\nxmin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$minimum\n[1] 0.3333333\n\n$objective\n[1] 0\n```\n:::\n:::\n\n\nNote how the (fixed) parameter `a` is passed into `f`.\n\n## Other `optimize()` arguments\n\n-   An interval within which to search must be specified, either by `interval` or by `upper` and `lower`\n\n-   To *maximise* set `maximum = TRUE`\n\n-   Accuracy can be set using the `tol` argument\n\n-   Note the order of arguments: \\`optimize(f, interval, ..., lower, upper, maximum, tol)\n\n-   The `...` can be named or unnamed and will be passed to `f`\n\n-   Arguments after the `...` must be specified by names.\n\n-   `optimize` returns a list with two items:\n\n    -   `minimum`: the value of `x` at which `f(x)` is minimised\n    -   `objective`: the value of `f(x)` at `x = minimum`\n\n## General Optimisation\n\n-   In more than one dimension the problem is harder.\n-   R has several different functions: most flexible is `optim()` which includes several different algorithms.\n-   Algorithm of choice depends on how easy it is to calculate derivatives for the function. Usually better to supply a function to calculate derivatives, but may be unnecessary extra work.\n-   Ensure the problem is scaled so that unit change in any parameter gives approximately unit change in objective.\n\n## `optim()` methods\n\n::: panel-tabset\n## Nelder-Mead\n\n-   The default method\n-   Basic idea: for a function with `n` parameters, choose a polygon with `n+1` vertices. At each step, alter vertex with minimum `f(x)` to improve the objective function, by *reflection*, *expansion* or *contraction*\n-   Does not use derivative information\n-   Useful for non-differentiable functions\n-   May be rather slow\n\n## BFGS\n\n-   A quasi-Newton method: builds up approximation to Hessian matrix from gradients at start and finish of steps\n-   Uses the approximation to choose new search direction\n-   Performs line search in this direction\n-   Update term for the Hessian approximation is due to Broyden, Fletcher, Goldfarb and Shanno (proposed separately by all four in 1970)\n-   Uses derivative information, calculated either from a user-supplied function or by finite differences\n-   If dimension is large, the matrix stored may be very large\n\n## CG method\n\n-   A conjugate gradient method: chooses successive search directions that are analogous to axes of an ellipse\n-   Does not store a Hessian matrix\n-   Three different formulae for the search directions are implemented: Fletcher-Reeves, Polak-Ribiere or Beale-Sorenson\n-   Less robust than BFGS method\n-   Uses derivative information, calculated either from a user-supplied function or by finite differences\n\n## L-BFGS-B\n\n-   A limited memory version of BFGS\n-   Does not store a Hessian matrix, only a limited number of update steps for it\n-   Uses derivative information, calculated either from a user-supplied function or by finite differences\n-   Can restrict the solution to lie within a \"box\", the only method of `optim()` that can do this\n\n## SANN\n\n-   A variant of simulated annealing A stochastic algorithm\n-   Accepts changes which increase the objective with positive probability (when minimising!)\n-   Does not use derivative information\n-   Can be very slow to converge but may find a 'good' solution quickly\n\n## Brent\n\n-   An interface to `optimize()`\n-   Only for use with one dimensional problems\n-   Included for use inside other functions where only method can be specified, not the function to be used.\n:::\n\n## How to use `optim()`\n\n`optim(par, fn, gr=NULL, ..., method=c(\"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\", \"Brent\"), lower=-Inf, upper=Inf, control=list(), hessian=FALSE)`\n\n. . .\n\n|                  |                                                                                            |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|\n| `par`            | starting values of the parameters                                                          |\n| `fn`             | function to be optimised (supply as for `optimize`)                                        |\n| `gr`             | function to calculate the derivative, only relevant for methods \"BFGS\", \"CG\" or \"L-BFGS-B\" |\n| `â€¦`              | other parameters for (both) `fn` and `gr`                                                  |\n| `method`         | algorithm to use                                                                           |\n| `lower`, `upper` | vector of limits for parameters (only allowed it `method=\"L-BFGS-B\"`                       |\n| `control`        | control options (see next slide)                                                           |\n| `hessian`        | logical: whether to return a hessian estimate calculated by finite differences             |\n\n## `optim`()`: control options\n\nThere are very many. The most important are:\n\nFILL IN TABLE\n\n## `optim()`: components of return value\n\nFILL IN TABLE\n\n## Constrained Optimisation\n\nINCLUDE THIS? WILL NEED TO LEARN IT!\n\n# End matter {background-color=\"#552D62\"}\n\n## Resources\n\nThis material is reproduced in large part from the APTS 2013/14 resources by Ruth Ripley:\n\n-   https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf\n\n## License\n\nLicensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License ([CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/){target=\"_blank\"}).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}