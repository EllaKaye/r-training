---
title: Simulation and Optimisation
subtitle: R Foundations course
author: Ella Kaye | Department of Statistics | University of Warwick
title-slide-attributes:
  data-background-color: "#552D62"
date: 2022-11-07
date-format: long
format: 
  revealjs:
    theme: [default, ../../slides.scss]
    slide-number: true
    execute:
      echo: true
      code-overflow: wrap
---

## Overview

-   Numerical precision

-   Random numbers

-   Simulation

-   Optimisation

# Numerical precision {background-color="#552D62"}

# Random numbers {background-color="#552D62"}

## Random number generation

- Random numbers caluclated on a computer are not random.

- They are *pseudo-random*, following a predicted sequence, but in the short-term (i.e. anything but the *very* long-term) will appear to be random.

- This is useful, as two sets of random numbers of the same size from the same generator using the same initial value will be exactly the same. **This is crucial for reproducibility.**

- R provides functions for generating random samples from standard distibutions.

## Using Random Number Generators in R

- Each time a random number is required, R will use and update a variable called `.Random.seed` which is in your workspace.
- At the first use, if `.Random.seed` does not exist, one will be created, with a value generated from the time.
- The function `set.seed(n)` will set `.Random.seed` to a value derived from the argument `n`. 
- Use `set.seed()` to repeat the same sequence of random numbers.
- There are various different generators available. R will use the one in use at the start of your session unless you alter it, even if you delete `.Random.seed`. For safety, you can specify the kind on calls to set.seed. Use `set.seed(n, kind="default")` to ensure you are using R’s default.
- It is possible to save and restore `.Random.seed` within your functions, but take care with scope (see next term!).

## `set.seed()` example

```{r}
sample(10)
set.seed(1)
sample(10)
sample(10)
set.seed(1)
sample(10)
```


## `sample()`

The `sample()` function re-samples from a data vector, with or without replacement.

::: {.panel-tabset}
## `sample(n)`

A random permutation of 1, ..., n

```{r}
set.seed(123)
sample(10)
```


## `sample(x)`

A random permutation of `x` for `length(x) > 1`

```{r}
set.seed(10)
alph10 <- LETTERS[1:10]
sample(alph10)
```

## sample(x, replace = TRUE)

A bootstrap sample

```{r}
set.seed(10)
sample(alph10, replace = TRUE)
```

## sample(x, n)

Sample `n` items from `x` without replacement

```{r}
set.seed(10)
sample(alph10, 6)
```

## sample(x, n, replace = TRUE)

Sample `n` items from `x` with replacement

```{r}
set.seed(1)
sample(alph10, 6, replace = TRUE)
```

## sample(x, n, replace = TRUE, prob = probs)

Probability sample of `n` items from `x`

```{r}
x <- 1:4
probs <- c(1/2, 1/3, 1/12, 1/12)

set.seed(1)
samp <- sample(x, 100, replace = TRUE, prob = probs)
samp
table(samp)
```


:::



# Simulation {background-color="#552D62"}

## Uses of Simulation

## Distributions in R

## Random samples from distributions

# Optimisation {background-color="#552D62"}

Slides below from [Ruth Ripley's APTS course](https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf)

## The optimisation problem

-   Given a function $f(x)$, what value of $x$ makes $f(x)$ as small or large as possible?
-   In a statistical context, $x$ will usually be the parameters of a model, and $f(x)$ either the model likelihood to be maximised or some measure of discrepancy between data and predictions to be minimised.
-   The optimal set of parameters will give the *best fit*
-   Only need to consider *small* as $-f(x)$ is *large* when $f(x)$ is *small*.
-   We consider here *general-purpose optimisers*

## Local and Global Minima

-   The (negative of the) likelihood for the General Linear Model (and that for many other linear models) is well-behaved: it has a single, global minimum.
-   For more complicated models there may be many local minima.
-   Finding a global minimum is difficult, and not always important. Only if local minima are widely separated in parameter space are they likely to invalidate our conclusions.
-   We will concentrate on methods of finding local minima. Check for different local minima by altering the initial values, algorithm used, or other parameters of the fitting process.

## Univariate Optimisation

`optimize` (or `optimise`) finds a (possibly local) mimimum of a function in a specified interval with respect to its first argument.

-   Function to be minimised is the first argument to `optimize`
-   Can pre-specify the function or include it in the command.

## Univariate Optimisation: example

```{r}
f <- function(x, a) {
  (x - a)^2
}

xmin <- optimize(f, interval = c(0, 1), a = 1/3)

# or

xmin <- optimize(function(x, a) {(x - a)^2}, 
                 interval = c(0, 1), a = 1/3)

xmin
```

Note how the (fixed) parameter `a` is passed into `f`.

## Other `optimize()` arguments

-   An interval within which to search must be specified, either by `interval` or by `upper` and `lower`

-   To *maximise* set `maximum = TRUE`

-   Accuracy can be set using the `tol` argument

-   Note the order of arguments: \`optimize(f, interval, ..., lower, upper, maximum, tol)

-   The `...` can be named or unnamed and will be passed to `f`

-   Arguments after the `...` must be specified by names.

-   `optimize` returns a list with two items:

    -   `minimum`: the value of `x` at which `f(x)` is minimised
    -   `objective`: the value of `f(x)` at `x = minimum`

## General Optimisation

-   In more than one dimension the problem is harder.
-   R has several different functions: most flexible is `optim()` which includes several different algorithms.
-   Algorithm of choice depends on how easy it is to calculate derivatives for the function. Usually better to supply a function to calculate derivatives, but may be unnecessary extra work.
-   Ensure the problem is scaled so that unit change in any parameter gives approximately unit change in objective.

## `optim()` methods

::: panel-tabset
## Nelder-Mead

-   The default method
-   Basic idea: for a function with `n` parameters, choose a polygon with `n+1` vertices. At each step, alter vertex with minimum `f(x)` to improve the objective function, by *reflection*, *expansion* or *contraction*
-   Does not use derivative information
-   Useful for non-differentiable functions
-   May be rather slow

## BFGS

-   A quasi-Newton method: builds up approximation to Hessian matrix from gradients at start and finish of steps
-   Uses the approximation to choose new search direction
-   Performs line search in this direction
-   Update term for the Hessian approximation is due to Broyden, Fletcher, Goldfarb and Shanno (proposed separately by all four in 1970)
-   Uses derivative information, calculated either from a user-supplied function or by finite differences
-   If dimension is large, the matrix stored may be very large

## CG method

-   A conjugate gradient method: chooses successive search directions that are analogous to axes of an ellipse
-   Does not store a Hessian matrix
-   Three different formulae for the search directions are implemented: Fletcher-Reeves, Polak-Ribiere or Beale-Sorenson
-   Less robust than BFGS method
-   Uses derivative information, calculated either from a user-supplied function or by finite differences

## L-BFGS-B

-   A limited memory version of BFGS
-   Does not store a Hessian matrix, only a limited number of update steps for it
-   Uses derivative information, calculated either from a user-supplied function or by finite differences
-   Can restrict the solution to lie within a "box", the only method of `optim()` that can do this

## SANN

-   A variant of simulated annealing A stochastic algorithm
-   Accepts changes which increase the objective with positive probability (when minimising!)
-   Does not use derivative information
-   Can be very slow to converge but may find a 'good' solution quickly

## Brent

-   An interface to `optimize()`
-   Only for use with one dimensional problems
-   Included for use inside other functions where only method can be specified, not the function to be used.
:::

## How to use `optim()`

`optim(par, fn, gr=NULL, ..., method=c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"), lower=-Inf, upper=Inf, control=list(), hessian=FALSE)`

. . .

|                  |                                                                                            |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|
| `par`            | starting values of the parameters                                                          |
| `fn`             | function to be optimised (supply as for `optimize`)                                        |
| `gr`             | function to calculate the derivative, only relevant for methods "BFGS", "CG" or "L-BFGS-B" |
| `…`              | other parameters for (both) `fn` and `gr`                                                  |
| `method`         | algorithm to use                                                                           |
| `lower`, `upper` | vector of limits for parameters (only allowed it `method="L-BFGS-B"`                       |
| `control`        | control options (see next slide)                                                           |
| `hessian`        | logical: whether to return a hessian estimate calculated by finite differences             |

## `optim`()`: control options

There are very many. The most important are:

FILL IN TABLE

## `optim()`: components of return value

FILL IN TABLE

## Constrained Optimisation

INCLUDE THIS? WILL NEED TO LEARN IT!

# End matter {background-color="#552D62"}

## Resources

This material is reproduced in large part from the APTS 2013/14 resources by Ruth Ripley:

-   https://portal.stats.ox.ac.uk/userdata/ruth/APTS2013/Rcourse5.pdf

## License

Licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License ([CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/){target="_blank"}).
